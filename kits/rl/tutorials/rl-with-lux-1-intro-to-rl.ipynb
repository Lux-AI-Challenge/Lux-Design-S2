{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Setup Code\n\nBefore we start lets install some dependencies","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"code","source":"# verify version\n!python --version\n!pip install --upgrade luxai_s2\n!pip install gym==0.19 pyglet\n!cp -r ../input/lux-ai-season-2/* .","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%writefile /opt/conda/lib/python3.7/site-packages/luxai_s2/version.py\n__version__ = \"\"","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Introduction to Reinforcement Learning ðŸ¤–\n\nLooking to try out RL for the Lux AI competition on Kaggle? If you haven't tried RL before, this tutorial is a perfect quick start into how RL generally works, and how to program a basic agent. This is part 1 of a 3 part series on training RL agents leading up to a succesful RL agent submittable to the Lux AI competition. Part 1 will cover the basics of RL, how gym environments work, and how to train an agent and scale up training.\n\n\nLet's dig in, welcome to the world of reinforcement learning!\n\nIn AI, RL is a framework of **learning via interaction**, often trying to **maximize the reinforcing reward**. Humans and animals alike sort of naturally follow the paradigm in learning behaviors. We reward a dog for doing a trick via a positive reward signal by giving it a treat. We penalize a dog for peeing on the floor by giving a negative reward signal (saying no!) or giving no reward. For a fun video of reinforcement learning you can watch this chicken below immediately learn to peck a particular colored dot via reinforcing it's actions with food.","metadata":{}},{"cell_type":"code","source":"from IPython.display import IFrame\nIFrame(\"https://www.youtube.com/embed/spfpBrBjntg\", 640, 480,)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"RL has seen a massive growth in applications in recent years thanks to the rise of deep learning resulting in Deep Reinforcement Learning. Deep learning has enabled models trained via RL to solve far more complex tasks, including [mining diamonds in Minecraft](https://danijar.com/project/dreamerv3/), [managing a nuclear fusion reactor](https://www.deepmind.com/blog/accelerating-fusion-science-through-learned-plasma-control), or [controlling a legged robot](https://manipulation-locomotion.github.io/).\n\n\nThis tutorial will cover the practical basics of RL and how to program a simple deep RL agent. For those more interested in some of the history, math, and more advanced details of RL I highly recommend reading the following seminal book on RL by Richard S. Sutton and Andrew G. Barto: http://www.incompleteideas.net/book/RLbook2020.pdf","metadata":{}},{"cell_type":"markdown","source":"## 1 The Environment Loop\n\nThe core component of RL is the environment loop. It's what enables an agent in an environment to repeatedly interact and improve itself.\n\nWe discretize time in an environment into individual **time steps** labeled as `t` below. At each time step, the agent uses the current environment observation (also known as state) $S_t$ to produce an action $A_t$ and executes that action in the environment. The environment updates in response to the action and gives the agent a new observation $S_{t+1}$ as well as a reward signal $R_{t+1}$ telling the agent how well it is doing. Eventually the environment will tell the agent it is completed and this completes one full episode.\n\n![](https://github.com/Lux-AI-Challenge/Lux-Design-S2/blob/main/kits/rl/tutorials/assets/rl_loop.jpg?raw=1)\n\nThese days, in deep RL the agent is typically a deep neural network that takes an observation as input and produces an action as output.\n\nWhile the agent is continously interacting, it will also periodically update its neural network via an optimization algorithm with a few objectives. Training in RL requires balancing exploration and exploitation. While the overall goal of the agent is to exploit and to **maximize return** (the sum of rewards in an episode), you can easily learn suboptimal behaviors if you don't explore sufficiently in an environment to find better strategies. This explore vs exploitation problem is a foundational problem in RL that is still researched to this day.\n\nFor a deep dive into the math and algorithms the [Spinning Up project](https://spinningup.openai.com/en/latest/) provides a great tutorial on some of the modern deep RL algorithms.\n","metadata":{}},{"cell_type":"markdown","source":"### 1.1 Environment Loop Code\n\nLet's get programming! The [Gym](https://github.com/openai/gym) package has now become the de-facto standard of the environment API and we will write code to show how to create an environment and interact with it here\n\n*Note: Recently the [Farama Foundation](https://farama.org/) which now manages the Gym package (now called [Gymnasium](https://www.gymlibrary.dev/)) has made several changes to the Gym API which is incompatible with most environments and RL libraries at the moment. This tutorial will be using the original API.*\n\nLet's first import a few packages.","metadata":{}},{"cell_type":"code","source":"import gym\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom PIL import Image","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"To create an environment with `gym`, any environemnt that is registered with gym can be created with `gym.make(env_id)`. To start looking at what it looks like you have to first call `env.reset()` to start from a clean state. It's highly recommended to also use `env.seed` to seed environment to ensure reproducible resultss.\n\n`env.render()` will render the environment with a display window (if possible). For users without access to GUI (e.g. on Google Colab, Kaggle notebooks etc.), you can call `env.render(\"rgb_array\")` to get an RGB image and display that image to see what the current state looks like\n\nFor this tutorial we will play around with the CartPole environment where the task is to keep the pole upright by simply moving the black box left and right.","metadata":{}},{"cell_type":"code","source":"env_id = \"CartPole-v1\"\nenv = gym.make(env_id)\nenv.seed(0)\nenv.reset()\nimg = env.render(\"rgb_array\")\nenv.close()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now let's try interacting with the environment. All environments will first define an observation space and action space. These spaces define the shape and structure of the observations returned by the environment and the actions it accepts.\n\nNeatly, the action space `env.action_space` allows you to randomly sample actions to try out. We will write a simple environment loop with random actions below. The loop will repeatedly ask for an action and we will step forward in the environment with that action via `env.step(action)`. \n\nYou might notice that `env.step` returns 4 items, `obs`, `reward`, `done`, and `info`. `obs` is the next observation. `reward` is the scalard reward signal given. `done` represents a somewhat ambiguous meaning. When it's `True` it means the episode is completed and you must call `env.reset()` before stepping through it again. Episode completion can occur for a number of reasons depending on the chosen environment. For CartPole-v0, `done` is `True` whenever a time limit is reached or if the pole falls down too far. Finally `info` usually is not important, but may contain some useful information depending on the environment.\n\nFor users without a GUI, we also provide a simple animation function to record and save videos","metadata":{}},{"cell_type":"code","source":"def animate(imgs, video_name=None, _return=True):\n    # using cv2 to generate videos\n    import cv2\n    import os\n    import string\n    import random\n    video_name = video_name if video_name is not None else ''.join(random.choice(string.ascii_letters) for i in range(18))+'.webm'\n    height, width, layers = imgs[0].shape\n    video = cv2.VideoWriter(video_name, cv2.VideoWriter_fourcc(*'VP80'), 20, (width,height))\n    for img in imgs:\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        video.write(img)\n    video.release()\n    if _return:\n        from IPython.display import Video\n        return Video(video_name)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# The environment loop\nobs = env.reset() # always reset before starting a new episode!\nimgs = []\nfor i in range(100):\n    action = env.action_space.sample() # sample a random action\n    obs, reward, done, info = env.step(action) # get the new observation and reward\n    imgs += [env.render(\"rgb_array\")] # save to video\n    if done: env.reset()\nenv.close() # close the display window and free up resources\nanimate(imgs, \"random_interaction.webm\") # generate the video replay","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2 Reinforcement Learning\n\nNow that we know how an environment works, we can try to solve it via RL. The optimization code for an RL algorithm is out of the scope of the tutorial so we will rely on a popular RL library called [Stable Baselines 3 (SB3)](https://github.com/DLR-RM/stable-baselines3). Run the command below to install it","metadata":{}},{"cell_type":"code","source":"!pip install stable-baselines3","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2.1 RL Training\nTraining RL algorithms for single-agent environments like CartPole is simple with SB3.\n\nThe algorithm we will use is called PPO. While the specifics of the algorithm are out of the scope, the general way an RL algorithm like PPO works is via a two stage process that constantly repeats.\n\n1. Collect interaction samples with sampled actions (observation, action, reward) from an environment and store into a replay buffer.\n\n2. Sample from the replay buffer and optimize the policy to maximize the return\n\nBelow is some example code which trains a policy by interacting for up to 10,000 timesteps with the environment and then evaluates it. You will notice that compared to using random actions, this policy can keep the pole upright for much longer (success!)","metadata":{}},{"cell_type":"code","source":"from stable_baselines3 import PPO\n\nenv = gym.make(\"CartPole-v1\")\n\n# create a PPO algorithm powered agent\nmodel = PPO(\"MlpPolicy\", env, verbose=1)\n# learn with a budget of 10,000 environment interactions\nmodel.learn(total_timesteps=10_000)\n\n# evaluate and watch the learned policy\nobs = env.reset()\nimgs = []\nfor i in range(1000):\n    action, _states = model.predict(obs, deterministic=True)\n    obs, reward, done, info = env.step(action)\n    imgs += [env.render(\"rgb_array\")] # save to video\n    # VecEnv resets automatically\n    if done:\n        obs = env.reset()\nenv.close()\nanimate(imgs, \"ppo_policy.webm\") # generate the video replay","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2.2 Scaling up Deep RL\n\nRL is often known for being a promising direction, but a fairly inefficient one. The particular algorithm used above is called PPO and has been a staple of the RL research community as it is generally easy to tune and very robust. However it is very **sample inefficient**, meaning it needs many many environment interactions in order to learn something. One way of dealing with this problem is to simply scale up the training by making it run faster. (The other way is to use different algorithms e.g. off-policy ones such as SAC but that's a topic beyond the scope of this tutorial)","metadata":{}},{"cell_type":"markdown","source":"A simple way to increase the speed at which we sample from an environment is to sample from many environments simultaneously. This enables us to leverage the power of parallel computation that is fast in neural networks and even faster when using a GPU/TPU. We can run `n_envs` environments simultaneously to form a single **Vectorized Environment**.\n\nVectorized environments **batch** an environment so that returned observations have an additional batch dimension (e.g. if it was shape `(3,)` it's new shape is `(B, 3)`) and accepted actions must also have this batch dimension, with `B` equal to the number of parallel environments. As vectorized environments accept a batch of actions, with GPUs/TPUs we can easily generate this batch of actions far faster than generating them one at a time, improving the speed at which we sample from an environment.\n\nTry changing the `n_envs` parameter below. You will notice a massive speed up in the training time as `n_envs` is larger. Note that `n_envs` shouldn't be higher than the number of cores your CPU has.","metadata":{}},{"cell_type":"code","source":"from stable_baselines3.common.env_util import make_vec_env\n\nn_envs = 2 # configure how many environments to run in parallel.\n\nenv = make_vec_env(\"CartPole-v1\", n_envs=n_envs)\nmodel = PPO(\"MlpPolicy\", env, verbose=1)\nmodel.learn(total_timesteps=10_000)\n\nobs = env.reset()\nimgs = []\nfor i in range(1000):\n    action, _states = model.predict(obs, deterministic=True)\n    obs, reward, done, info = env.step(action)\n    imgs += [env.render(\"rgb_array\")] # save to video\n    # VecEnv resets automatically so no need to do below\n    # if done: env.reset()\nenv.close()\nanimate(imgs, \"ppo_policy_vec_env.webm\") # generate the video replay","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3 Final Thoughts\n\nAnd that's the basics of programming a deep RL agent! Pick an environment, vectorize it, and run it through a RL library like SB3.\n\nFor more complicated environments like robotics or multi-agent environments there's a lot more work involved. This may include **reward shaping**, **hyperparameter tuning**, as well as \n\nPart 2 of the tutorial series will these details on how to use RL to tackle parts of the Lux AI Season2 environment.","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}